{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2cce91-0803-40d6-a365-cd32299240e5",
   "metadata": {},
   "source": [
    "# Evaluation on each checkpoint of the NER Finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f577a35-2ee3-464b-be79-a630a02a0032",
   "metadata": {},
   "source": [
    "## Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b9a8ff-cc28-4fdc-8790-915ac3f43ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "def load_finetuned_model(model_path, model_id, num_labels=3):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label={0: \"O\", 1: \"B\", 2: \"I\"},\n",
    "        label2id={\"O\": 0, \"B\": 1, \"I\": 2},\n",
    "        local_files_only=True  # ðŸ§  This tells it not to look on the Hugging Face Hub\n",
    "    )\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "# Predict function\n",
    "def predict_ner(tokens, model, tokenizer, device):\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    word_ids = inputs.word_ids()\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predictions = logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    id2label = {0: \"O\", 1: \"B\", 2: \"I\"}\n",
    "    word_preds = []\n",
    "    prev_word_id = None\n",
    "    current_preds = []\n",
    "\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        if word_id != prev_word_id:\n",
    "            if current_preds:\n",
    "                word_preds.append(id2label[max(set(current_preds), key=current_preds.count)])\n",
    "            current_preds = [predictions[i]]\n",
    "            prev_word_id = word_id\n",
    "        else:\n",
    "            current_preds.append(predictions[i])\n",
    "\n",
    "    if current_preds:\n",
    "        word_preds.append(id2label[max(set(current_preds), key=current_preds.count)])\n",
    "\n",
    "    return word_preds\n",
    "\n",
    "# Evaluation pipeline\n",
    "def evaluate_model(model_path, model_id=\"meta-llama/Llama-3.2-3B-Instruct\", num_samples=None):\n",
    "    model, tokenizer, device = load_finetuned_model(model_path, model_id)\n",
    "\n",
    "    dataset = load_dataset(\"spyysalo/species_800\")[\"test\"]\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(num_samples))\n",
    "\n",
    "    label_list = [\"O\", \"B\", \"I\"]\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    print(f\"Evaluating on {len(dataset)} samples...\")\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        tokens = example[\"tokens\"]\n",
    "        true_labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
    "        pred_labels = predict_ner(tokens, model, tokenizer, device)\n",
    "\n",
    "        min_len = min(len(true_labels), len(pred_labels))\n",
    "        y_true.append(true_labels[:min_len])\n",
    "        y_pred.append(pred_labels[:min_len])\n",
    "\n",
    "    # Metrics\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "\n",
    "    # Output\n",
    "    print(\"\\n--- Final Evaluation Metrics ---\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall:    {recall:.3f}\")\n",
    "    print(f\"F1 Score:  {f1:.3f}\")\n",
    "    print(\"\\nDetailed Report:\\n\", report)\n",
    "\n",
    "    # Save summary\n",
    "    pd.DataFrame([{\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }]).to_csv(\"finetuned_species_ner_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7acbf8-95fb-4967-970b-6e06a9814183",
   "metadata": {},
   "source": [
    "### Checkpoint 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ae0508-0452-4f84-8984-68427baed9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc3456426f44304a188b941225b6380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not load bitsandbytes native library: /usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/ctypes/__init__.py\", line 454, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: /usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1631 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [01:11<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Metrics ---\n",
      "Precision: 0.462\n",
      "Recall:    0.501\n",
      "F1 Score:  0.480\n",
      "\n",
      "Detailed Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.46      0.50      0.48       767\n",
      "\n",
      "   micro avg       0.46      0.50      0.48       767\n",
      "   macro avg       0.46      0.50      0.48       767\n",
      "weighted avg       0.46      0.50      0.48       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_path=\"./NER_finetuned_models/species_ner_model_llama_3-2-3B/checkpoint-360\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d62741-4c73-4cc9-a5cf-3d3f879cabdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c46ee69c774b2cb978467b8be31668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1631 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [01:04<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Metrics ---\n",
      "Precision: 0.469\n",
      "Recall:    0.536\n",
      "F1 Score:  0.500\n",
      "\n",
      "Detailed Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.47      0.54      0.50       767\n",
      "\n",
      "   micro avg       0.47      0.54      0.50       767\n",
      "   macro avg       0.47      0.54      0.50       767\n",
      "weighted avg       0.47      0.54      0.50       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_path=\"./NER_finetuned_models/species_ner_model_llama_3-2-3B/checkpoint-720\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5a0b57-1af8-4bfc-a590-597e4692dcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318c30c023e54d25b8f291478325de9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1631 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [01:03<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Metrics ---\n",
      "Precision: 0.462\n",
      "Recall:    0.501\n",
      "F1 Score:  0.480\n",
      "\n",
      "Detailed Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.46      0.50      0.48       767\n",
      "\n",
      "   micro avg       0.46      0.50      0.48       767\n",
      "   macro avg       0.46      0.50      0.48       767\n",
      "weighted avg       0.46      0.50      0.48       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_path=\"./NER_finetuned_models/species_ner_finetuned_model_llama_3-2-3B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942134bc-f042-42e3-9672-ada30841fe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d0fe6978d744daa934fbd7bbbdff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1631 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [01:04<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Metrics ---\n",
      "Precision: 0.444\n",
      "Recall:    0.437\n",
      "F1 Score:  0.440\n",
      "\n",
      "Detailed Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.44      0.44      0.44       767\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       767\n",
      "   macro avg       0.44      0.44      0.44       767\n",
      "weighted avg       0.44      0.44      0.44       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_path=\"./NER_finetuned_models/species_ner_model_llama_3-2-3B/checkpoint-1080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bcb562c-eb9c-4e5e-88f9-cb0326a449ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cea7d816351417dbb32a6890917ff15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not load bitsandbytes native library: /usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/ctypes/__init__.py\", line 454, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/envs/mahshid_thesis/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: /usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /root/envs/mahshid_thesis/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1631 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [01:03<00:00, 25.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Metrics ---\n",
      "Precision: 0.387\n",
      "Recall:    0.399\n",
      "F1 Score:  0.393\n",
      "\n",
      "Detailed Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.39      0.40      0.39       767\n",
      "\n",
      "   micro avg       0.39      0.40      0.39       767\n",
      "   macro avg       0.39      0.40      0.39       767\n",
      "weighted avg       0.39      0.40      0.39       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_path=\"./NER_finetuned_models/species_ner_model_llama_3-2-3B/checkpoint-180\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8b4f03-bcd1-4366-876a-1fecf2066265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e0b2c6df194bc9a29a95708e79619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1631 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [01:04<00:00, 25.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Metrics ---\n",
      "Precision: 0.441\n",
      "Recall:    0.421\n",
      "F1 Score:  0.431\n",
      "\n",
      "Detailed Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.44      0.42      0.43       767\n",
      "\n",
      "   micro avg       0.44      0.42      0.43       767\n",
      "   macro avg       0.44      0.42      0.43       767\n",
      "weighted avg       0.44      0.42      0.43       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_path=\"./NER_finetuned_models/species_ner_model_llama_3-2-3B/checkpoint-540\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2786e7df-624b-4e1f-87e7-12727bf378fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0770b47da37745fd9cf40744313f9cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1631 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [01:03<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Metrics ---\n",
      "Precision: 0.438\n",
      "Recall:    0.426\n",
      "F1 Score:  0.432\n",
      "\n",
      "Detailed Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.44      0.43      0.43       767\n",
      "\n",
      "   micro avg       0.44      0.43      0.43       767\n",
      "   macro avg       0.44      0.43      0.43       767\n",
      "weighted avg       0.44      0.43      0.43       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_path=\"./NER_finetuned_models/species_ner_model_llama_3-2-3B/checkpoint-900\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f9644-d06a-4466-8da1-e7e65f8b1ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
